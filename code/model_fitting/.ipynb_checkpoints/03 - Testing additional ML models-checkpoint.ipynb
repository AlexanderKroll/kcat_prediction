{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.preview in file CCB_plot_style_0v4.mplstyle, line 55 ('text.latex.preview  : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file CCB_plot_style_0v4.mplstyle, line 63 ('mathtext.fallback_to_cm : True ## When True, use symbols from the Computer Modern fonts')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "from tensorflow.keras import regularizers, initializers, optimizers, models, layers\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.activations import relu\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib as mpl\n",
    "plt.style.use('CCB_plot_style_0v4.mplstyle')\n",
    "c_styles      = mpl.rcParams['axes.prop_cycle'].by_key()['color']   # fetch the defined color styles\n",
    "high_contrast = ['#004488', '#DDAA33', '#BB5566', '#000000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3421, 850)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"train_df_kcat.pkl\"))\n",
    "data_test = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"test_df_kcat.pkl\"))\n",
    "\n",
    "\n",
    "data_train.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n",
    "data_test.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n",
    "len(data_train), len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(np.load(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"CV_train_indices.npy\"), allow_pickle = True))\n",
    "test_indices = list(np.load(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"CV_test_indices.npy\"), allow_pickle = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"DRFP\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM1b_ts\"]))], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"DRFP\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM1b_ts\"]))], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_y, std_y = np.mean(train_Y), np.std(train_Y)\n",
    "train_Y = (train_Y-mean_y)/std_y\n",
    "test_Y = (test_Y-mean_y)/std_y\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_X[:, 2048:])\n",
    "train_X[:, 2048:] = scaler.transform(train_X[:, 2048:])\n",
    "test_X[:, 2048:] = scaler.transform(test_X[:, 2048:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training and validation machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Performing hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_neg_r2_linear_regression(param):\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "\n",
    "        reg = ElasticNet(alpha = param[\"alpha\"], l1_ratio = param[\"l1_ratio\"]).fit(train_X[train_index], train_Y[train_index])\n",
    "        y_valid_pred = reg.predict(train_X[test_index])\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "#Defining search space for hyperparameter optimizationhp.uniform(\"reg_alpha\", 0, 5)\n",
    "space_linear_regression = {'alpha': hp.uniform('alpha', 0,5),\n",
    "                            'l1_ratio': hp.uniform('l1_ratio', 0,1)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trials = Trials()\n",
    "best = fmin(fn = cross_validation_neg_r2_linear_regression, space = space_linear_regression,\n",
    "            algo=rand.suggest, max_evals = 2000, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param = trials.argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'alpha': 0.3960857176137572, 'l1_ratio': 0.003735725013911728}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Training and validating the final model\n",
    "Training the model and validating it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y = (test_Y+mean_y)*std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.542 1.014 0.293\n"
     ]
    }
   ],
   "source": [
    "reg = ElasticNet(alpha = param[\"alpha\"], l1_ratio = param[\"l1_ratio\"]).fit(train_X, train_Y)\n",
    "y_test_pred = reg.predict(test_X)\n",
    "y_test_pred = (y_test_pred+mean_y)*std_y\n",
    "\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input matrices:\n",
    "train_X = np.array(list(data_train[\"DRFP\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM1b_ts\"]))], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"DRFP\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM1b_ts\"]))], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_X)\n",
    "train_X = scaler.transform(train_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_neg_r2_random_forest(param):\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "\n",
    "        reg = RandomForestRegressor(max_depth = param[\"max_depth\"],\n",
    "                                    min_samples_leaf = param[\"min_samples_leaf\"],\n",
    "                                    n_estimators = param[\"n_estimators\"]).fit(train_X[train_index], train_Y[train_index])\n",
    "        y_valid_pred = reg.predict(train_X[test_index])\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "#Defining search space for hyperparameter optimization\n",
    "space_random_forest = {'n_estimators': hp.choice('n_estimators', [50, 100, 200]),\n",
    "                      'max_depth': hp.choice('max_depth', [5,6,7,8,9,10,11,12,13,14,15,16]),\n",
    "                       'min_samples_leaf': hp.choice('min_samples_leaf', [1,2,5,10,20])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trials = Trials()\n",
    "best = fmin(fn = cross_validation_neg_r2_random_forest, space = space_random_forest,\n",
    "            algo=rand.suggest, max_evals = 2000, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trials.argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 15, 'min_samples_leaf': 1, 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Training and validating the final model\n",
    "Training the model and validating it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.622 0.911 0.364\n"
     ]
    }
   ],
   "source": [
    "reg = RandomForestRegressor(max_depth = param[\"max_depth\"],\n",
    "                                    min_samples_leaf = param[\"min_samples_leaf\"],\n",
    "                                    n_estimators = param[\"n_estimators\"]).fit(train_X, train_Y)\n",
    "y_test_pred = reg.predict(test_X)\n",
    "\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"DRFP\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM1b_ts\"]))], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"DRFP\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM1b_ts\"]))], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))\n",
    "\n",
    "mean_y, std_y = np.mean(train_Y), np.std(train_Y)\n",
    "train_Y = (train_Y-mean_y)/std_y\n",
    "test_Y = (test_Y-mean_y)/std_y\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_X[:, 2048:])\n",
    "train_X[:, 2048:] = scaler.transform(train_X[:, 2048:])\n",
    "test_X[:, 2048:] = scaler.transform(test_X[:, 2048:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(learning_rate=0.001, decay =10e-6, momentum=0.9, l2_parameter= 0.1, hidden_layer_size1 = 256,\n",
    "               hidden_layer_size2 = 64, input_dim = 1024, third_layer = True): \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(units = hidden_layer_size1,\n",
    "                           kernel_regularizer=regularizers.l2(l2_parameter),\n",
    "                           kernel_initializer = initializers.TruncatedNormal(\n",
    "                               mean=0.0, stddev= np.sqrt(2./ input_dim), seed=None),\n",
    "                           activation='relu', input_shape=(input_dim,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(units= hidden_layer_size2,\n",
    "                           kernel_regularizer=regularizers.l2(l2_parameter),\n",
    "                           kernel_initializer = initializers.TruncatedNormal(\n",
    "                               mean=0.0, stddev = np.sqrt(2./ hidden_layer_size1), seed=None),\n",
    "                           activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    if third_layer == True:\n",
    "        model.add(layers.Dense(units= 16,\n",
    "                               kernel_regularizer=regularizers.l2(l2_parameter),\n",
    "                               kernel_initializer = initializers.TruncatedNormal(\n",
    "                                   mean=0.0, stddev = np.sqrt(2./ hidden_layer_size2), seed=None),\n",
    "                               activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "     \n",
    "    model.add(layers.Dense(1, kernel_regularizer=regularizers.l2(l2_parameter),\n",
    "                           kernel_initializer = initializers.TruncatedNormal(\n",
    "                               mean=0.0, stddev = np.sqrt(2./ 16), seed=None)))\n",
    "    model.compile(optimizer=optimizers.SGD(learning_rate=learning_rate,  momentum=momentum, nesterov=True),\n",
    "                  loss='mse',  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_neg_r2_fcnn(param):\n",
    "    \n",
    "    param[\"num_epochs\"] = int(np.round(param[\"num_epochs\"]))\n",
    "\n",
    "    \n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        model = build_model(input_dim = 1280+2048, \n",
    "                            learning_rate= param[\"learning_rate\"],\n",
    "                            decay = param[\"decay\"],\n",
    "                            momentum = param[\"momentum\"], \n",
    "                            l2_parameter = param[\"l2_parameter\"],\n",
    "                            hidden_layer_size1 = param[\"hidden_layer_size1\"],\n",
    "                            hidden_layer_size2 = param[\"hidden_layer_size2\"]) \n",
    "\n",
    "        model.fit(np.array(train_X[train_index]), np.array(train_Y[train_index]),\n",
    "                            epochs = param[\"num_epochs\"],\n",
    "                            batch_size = param[\"batch_size\"],\n",
    "                            verbose=0)\n",
    "\n",
    "        R2.append(r2_score( np.reshape(train_Y[test_index], (-1)),\n",
    "                           model.predict(np.array(train_X[test_index])).reshape(-1) ))\n",
    "    return(-np.mean(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''space = {\"learning_rate\": hp.uniform(\"learning_rate\", 1e-6, 1e-2),\n",
    "        \"hidden_layer_size1\": hp.choice(\"hidden_layer_size1\", [256,128,64]),\n",
    "        \"hidden_layer_size2\": hp.choice(\"hidden_layer_size2\", [128,64,32]),\n",
    "        \"batch_size\": hp.choice(\"batch_size\", [8,16,32,64,96]),\n",
    "        \"decay\": hp.uniform(\"decay\", 1e-9, 1e-5),\n",
    "        \"l2_parameter\": hp.uniform(\"l2_parameter\", 0, 0.01),\n",
    "        \"momentum\": hp.uniform(\"momentum\", 0.1, 1),\n",
    "        \"num_epochs\": hp.uniform(\"num_epochs\", 20, 100)}\n",
    "    \n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_neg_r2_fcnn, space = space, algo=rand.suggest, max_evals= 500, trials=trials)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'batch_size': 96,\n",
    "         'decay': 8.925865617547346e-06,\n",
    "         'hidden_layer_size1': 128,\n",
    "         'hidden_layer_size2': 64,\n",
    "         'l2_parameter': 0.0033008915899278156,\n",
    "         'learning_rate': 0.006808549614442447,\n",
    "         'momentum': 0.9054104435951468,\n",
    "         'num_epochs': 62.68663708309369}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "36/36 [==============================] - 1s 6ms/step - loss: 2.0003 - mse: 0.9252\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.6849 - mse: 0.6270\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.5191 - mse: 0.4923\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.3920 - mse: 0.3958\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.3207 - mse: 0.3543\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.2364 - mse: 0.2991\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.1669 - mse: 0.2581\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.1248 - mse: 0.2438\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0808 - mse: 0.2267\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0345 - mse: 0.2062\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9915 - mse: 0.1887\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9617 - mse: 0.1836\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9230 - mse: 0.1688\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8986 - mse: 0.1674\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8757 - mse: 0.1667\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8451 - mse: 0.1576\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8030 - mse: 0.1366\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7789 - mse: 0.1331\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7582 - mse: 0.1323\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7438 - mse: 0.1367\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7180 - mse: 0.1293\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6865 - mse: 0.1157\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6692 - mse: 0.1160\n",
      "Epoch 24/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6430 - mse: 0.1068\n",
      "Epoch 25/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6295 - mse: 0.1097\n",
      "Epoch 26/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6212 - mse: 0.1172\n",
      "Epoch 27/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6002 - mse: 0.1114\n",
      "Epoch 28/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5760 - mse: 0.1018\n",
      "Epoch 29/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5613 - mse: 0.1014\n",
      "Epoch 30/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5434 - mse: 0.0974\n",
      "Epoch 31/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5324 - mse: 0.0997\n",
      "Epoch 32/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5300 - mse: 0.1103\n",
      "Epoch 33/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4996 - mse: 0.0924\n",
      "Epoch 34/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4970 - mse: 0.1019\n",
      "Epoch 35/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4783 - mse: 0.0948\n",
      "Epoch 36/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4584 - mse: 0.0863\n",
      "Epoch 37/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4553 - mse: 0.0943\n",
      "Epoch 38/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4456 - mse: 0.0950\n",
      "Epoch 39/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4338 - mse: 0.0934\n",
      "Epoch 40/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4173 - mse: 0.0868\n",
      "Epoch 41/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4133 - mse: 0.0924\n",
      "Epoch 42/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3981 - mse: 0.0865\n",
      "Epoch 43/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3945 - mse: 0.0917\n",
      "Epoch 44/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3867 - mse: 0.0925\n",
      "Epoch 45/50\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3825 - mse: 0.0965\n",
      "Epoch 46/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3695 - mse: 0.0916\n",
      "Epoch 47/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3576 - mse: 0.0874\n",
      "Epoch 48/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3501 - mse: 0.0874\n",
      "Epoch 49/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3387 - mse: 0.0833\n",
      "Epoch 50/50\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3314 - mse: 0.0830\n",
      "27/27 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3237192981547061"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(input_dim = 1280+2048, \n",
    "                            learning_rate = param[\"learning_rate\"],\n",
    "                            decay = param[\"decay\"],\n",
    "                            momentum = param[\"momentum\"], \n",
    "                            l2_parameter = param[\"l2_parameter\"], \n",
    "                            hidden_layer_size1 = param[\"hidden_layer_size1\"],\n",
    "                            hidden_layer_size2 = param[\"hidden_layer_size2\"]) \n",
    "\n",
    "model.fit(np.array(train_X), np.array(train_Y),\n",
    "                    epochs = 50,# int(np.round(param[\"num_epochs\"])),\n",
    "                    batch_size = param[\"batch_size\"],\n",
    "                    verbose=1)\n",
    "\n",
    "y_test_pred = model.predict(np.array(test_X))\n",
    "r2_score(test_Y, y_test_pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.593 0.969 0.324\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = (y_test_pred.reshape(-1) + mean_y)*std_y\n",
    "test_Y = (test_Y + mean_y)*std_y\n",
    "\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred.reshape(-1))**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred.reshape(-1))\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred.reshape(-1))\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
